{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c2cd5e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c00fda",
   "metadata": {},
   "source": [
    "# Re-install the environment\n",
    "deactivate\n",
    "rm -rf venv\n",
    "\n",
    "/opt/homebrew/bin/python3.10 -m venv venv\n",
    "\n",
    "source venv/bin/activate\n",
    "python --version\n",
    "pip install --upgrade pip\n",
    "pip install langchain google-generativeai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76582aaa",
   "metadata": {},
   "source": [
    "Environment setup \n",
    "- Install local `pyenv` for local environment management.\n",
    "- Set the local directory to the desired python version.\n",
    "- Install the `Langchain` package.\n",
    "- Start the virtual environment `source ~/Documents/github/ai-agent/venv/bin/activate`.\n",
    "- Use `pip install ipykernel`.  \n",
    "    - `ipykerne`l` is Jupyter's tool for connecting to specific Python environments.\n",
    "    - It allows `Jupyter Notebook` to use the Python interpreters and libraries in the virtual environment.\n",
    "- `jupyter kernelspec list` Verify that the kernel is installed:\n",
    "\n",
    "\n",
    "環境設定 \n",
    "- ローカル環境を管理するために、ローカルの `pyenv` をインストールする。\n",
    "- ローカルのディレクトリを目的の python のバージョンに設定する。\n",
    "- Langchain` パッケージをインストールする。\n",
    "- 仮想環境 `source ~/Documents/github/ai-agent/venv/bin/activate` を起動する。\n",
    "- pip install ipykernel` を使用する。 \n",
    "    - ipykerne`l` は特定のPython環境に接続するためのJupyterのツールである。\n",
    "    - これによって `Jupyter Notebook` は仮想環境のPythonインタプリタとライブラリを利用できるようになる。\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "654baa96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\n"
     ]
    }
   ],
   "source": [
    "#! pip install --upgrade langchain -i https://pypi.org/simple\n",
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e42457",
   "metadata": {},
   "source": [
    "# Langchain example\n",
    "1. create LLM\n",
    "2. Custom Prompt Template \n",
    "- Context Templating Approach:\n",
    "- Support dynamic input parameters:\n",
    "    - Subject/topic\n",
    "    - Difficulty level\n",
    "    - Number of questions\n",
    "    - Question type (multiple choice, true/false, etc.)\n",
    "3. Output Interpreter\n",
    "- Advanced Parsing Capabilities:\n",
    "    - Convert LLM output to structured formats\n",
    "    - Support for multiple output types:\n",
    "        - JSON\n",
    "        - Dictionary\n",
    "        - Structured list\n",
    "- Implement error handling and validation\n",
    "4. First Implementation\n",
    "- Interactive Quiz Generation\n",
    "- Dynamic Parameters:\n",
    "    - Subject selection\n",
    "    - Difficulty level\n",
    "    - Number of questions\n",
    "- Flexible Input Handling\n",
    "- Structured Output Generation\n",
    "\n",
    "- `LLM.predict()` predict(): Generates next tokens\n",
    "\n",
    "- `langchain.prompts`\n",
    "- `prompt.format(category=\"\")`\n",
    "- `langchain.schema` \n",
    "- `OutputParser.parse()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dbb95413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-2.5-pro-exp-03-25\n",
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/learnlm-1.5-pro-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# 列出可用模型\n",
    "for m in genai.list_models():\n",
    "    if 'generateContent' in m.supported_generation_methods:\n",
    "        print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3f0627eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/itsyuimorii/Documents/github-new/ai-agent-langchain/new_venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "# export GOOGLE_API_KEY=AIzaSyBx6_WRVw51fTN5TBwfzaFkpXqh0LOr1us\n",
    "\n",
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0a30b30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The coder toiled, both late and long,\\nWith APIs, a tangled throng.\\nHe dreamt of chains, not made of steel,\\nBut links of thought, that could reveal\\nThe wisdom hidden, deep inside\\nThe data\\'s vast and surging tide.\\n\\nThen from the depths, a whisper came,\\nA framework bright, a burning flame.\\nLangChain, it called, a binding force,\\nTo chart a new and different course.\\nNo longer bound by single calls,\\nIt broke the walls, and scaled the halls.\\n\\nIt linked the models, large and small,\\nFrom OpenAI, standing tall,\\nTo Hugging Face, and many more,\\nA symphony of learned lore.\\nIt chained the prompts, step by step,\\nAnd secrets whispered, it could intercept.\\n\\nThe coder wept, with joyful tears,\\nThe end of toil, the end of fears.\\nHe built his agents, keen and bright,\\nThat reasoned, planned, with all their might.\\nThey answered questions, clear and true,\\nAnd tasks they did, both old and new.\\n\\nBut whispers warned, a cautious plea,\\n\"With power great, comes duty,\" see.\\n\"The chains you forge, with careful hand,\\nCan shape the future, and command.\\nUse wisdom\\'s light, and ethics bold,\\nLest stories dark, the future hold.\"\\n\\nThe coder paused, his heart ablaze,\\nHe knew the power, and the maze.\\nHe swore an oath, both strong and deep,\\nThe chains of thought, he\\'d safely keep.\\nAnd so he built, with purpose true,\\nA future bright, for me and you,\\nWith LangChain\\'s help, the world anew.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-pro-002', 'safety_ratings': []} id='run-5a1cbe03-f722-4a0b-9cf5-1bcad8cfb241-0' usage_metadata={'input_tokens': 7, 'output_tokens': 364, 'total_tokens': 371, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # This loads variables from .env file\n",
    "\n",
    "# Now use LangChain as normal - it will find the key in environment\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "response = llm.invoke(\"Write me a ballad about LangChain\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2a5a2989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! I am a large language model, trained by Google. I am designed to provide information and complete tasks as instructed. I can generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way. I am still under development, but I have learned to perform many kinds of tasks, including:\\n\\n*   **Answering questions:** I can answer questions on a wide range of topics, drawing on the vast amount of information I have been trained on.\\n*   **Generating text:** I can generate different creative text formats, like poems, code, scripts, musical pieces, email, letters, etc. I will try my best to fulfill all your requirements.\\n*   **Translating languages:** I can translate text from one language to another.\\n*   **Summarizing text:** I can provide concise summaries of longer pieces of text.\\n*   **Following instructions:** I can follow your instructions and complete tasks as requested.\\n\\nHow can I help you today?'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(  model=\"gemini-2.0-flash-001\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")\n",
    "llm.predict(\"Introduce yourself\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb48e4b",
   "metadata": {},
   "source": [
    "# Using `LangChain` Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9e594a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a PDF reader, read the following PDF and answer the question. Hello, how are you?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Please provide me with the PDF file. I need the content of the PDF to be able to read it and answer your question. I cannot access files from your computer or the internet without you providing them to me.\\n\\nOnce you provide the PDF content, I will do my best to read it and answer your question.'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(  model=\"gemini-2.0-flash-001\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")\n",
    "\n",
    "prompt= PromptTemplate.from_template(\"You are a PDF reader, read the following PDF and answer the question. {pdf_text}\")\n",
    "message = prompt.format(pdf_text=\"Hello, how are you?\")\n",
    "print(message)\n",
    "llm.predict(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0222677",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6993fc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/itsyuimorii/Documents/github-new/ai-agent-langchain/new_venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2797b4be",
   "metadata": {},
   "source": [
    "# PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cce7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# from langchain_community.document_loaders import PyPDFLoader\n",
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('/opt/homebrew/lib/python3.10/site-packages')\n",
    "\n",
    "# # Now your imports should work\n",
    "# from langchain_community.document_loaders import PyPDFLoader\n",
    "# # Load environment variables (including your Google API key)\n",
    "# load_dotenv()\n",
    "\n",
    "# # Set the path to your PDF file\n",
    "# file_path = \"/Users/itsyuimorii/Downloads/Digital Transformation.pdf\"\n",
    "\n",
    "# # Load and parse the PDF\n",
    "# loader = PyPDFLoader(file_path)\n",
    "# pages = loader.load_and_split()\n",
    "\n",
    "# # Print the number of pages found\n",
    "# print(f\"Loaded {len(pages)} pages from the PDF\")\n",
    "\n",
    "# # Initialize the Gemini model\n",
    "# llm = ChatGoogleGenerativeAI(\n",
    "#     model=\"gemini-2.0-flash-001\",\n",
    "#     temperature=0,\n",
    "#     max_tokens=None\n",
    "# )\n",
    "\n",
    "# # If you want to analyze specific content from the PDF\n",
    "# if len(pages) > 0:\n",
    "#     # Example: Summarize the first page\n",
    "#     question = f\"Summarize the following content from a PDF about Digital Transformation: {pages[0].page_content[:1000]}\"\n",
    "#     response = llm.invoke(question)\n",
    "#     print(\"\\nSummary of first page:\")\n",
    "#     print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8e5f27",
   "metadata": {},
   "source": [
    "# Formatting output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "115b6648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ' how are you?']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "# Self-defining class, inherits BaseOutputParser.\n",
    "class CommaSeperatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "\n",
    "    def parse(self, text: str) -> list[str]:\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\",\")\n",
    "    \n",
    "CommaSeperatedListOutputParser().parse(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "91e70094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import AIMessagePromptTemplate\n",
    "from langchain.prompts import SystemMessagePromptTemplate\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain.prompts import ChatMessagePromptTemplate\n",
    "\n",
    "prompt = \"Dif-tor heh smusma-Live long and prosper {subject}\"\n",
    "\n",
    "chat_message_prompt = AIMessagePromptTemplate.from_template(template=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4551cfb",
   "metadata": {},
   "source": [
    "#  Idea01: PDF processing and Gemini to create a chatbox\n",
    "Using LangChain in conjunction with PDF processing and a large language model such as Gemini to create a chatbox that can talk to PDF documents is a great direction to take. You've got a good starting point, so let me explain further how to accomplish this.\n",
    "To create a chatbox that can talk to PDFs, you need to add the following key components:\n",
    "\n",
    "Vector storage: Convert PDF content into vectors and store them so that semantic search is possible\n",
    "Retrieval system: retrieve relevant content based on user questions\n",
    "Conversation Memory: Keeps conversations in context\n",
    "Conversation Chaining: Connects all components into a complete conversation system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8fa0df",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File path /Users/itsyuimoriispace/Desktop/Digital Transformation.pdf is not a valid file or url",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/itsyuimoriispace/Desktop/Digital Transformation.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Load and parse the PDF\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m loader \u001b[38;5;241m=\u001b[39m \u001b[43mPyPDFLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m pages \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload_and_split()\n\u001b[1;32m     25\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatGoogleGenerativeAI(\n\u001b[1;32m     26\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-2.0-flash-001\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     max_retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     31\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/github-new/ai-agent-langchain/new_venv/lib/python3.10/site-packages/langchain_community/document_loaders/pdf.py:281\u001b[0m, in \u001b[0;36mPyPDFLoader.__init__\u001b[0;34m(self, file_path, password, headers, extract_images, mode, images_parser, images_inner_format, pages_delimiter, extraction_mode, extraction_kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    240\u001b[0m     file_path: Union[\u001b[38;5;28mstr\u001b[39m, PurePath],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m     extraction_kwargs: Optional[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    251\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize with a file path.\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;03m        `aload` methods to retrieve parsed documents with content and metadata.\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser \u001b[38;5;241m=\u001b[39m PyPDFParser(\n\u001b[1;32m    283\u001b[0m         password\u001b[38;5;241m=\u001b[39mpassword,\n\u001b[1;32m    284\u001b[0m         mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    290\u001b[0m         extraction_kwargs\u001b[38;5;241m=\u001b[39mextraction_kwargs,\n\u001b[1;32m    291\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/github-new/ai-agent-langchain/new_venv/lib/python3.10/site-packages/langchain_community/document_loaders/pdf.py:140\u001b[0m, in \u001b[0;36mBasePDFLoader.__init__\u001b[0;34m(self, file_path, headers)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(temp_pdf)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path):\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile path \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not a valid file or url\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path)\n",
      "\u001b[0;31mValueError\u001b[0m: File path /Users/itsyuimoriispace/Desktop/Digital Transformation.pdf is not a valid file or url"
     ]
    }
   ],
   "source": [
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# from langchain.prompts import PromptTemplate\n",
    "# from langchain_community.document_loaders import PyPDFLoader\n",
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "# from langchain.schema import BaseOutputParser\n",
    "\n",
    "# class CommaSeperatedListOutputParser(BaseOutputParser):\n",
    "#     \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "\n",
    "#     def parse(self, text: str) -> list[str]:\n",
    "#         \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "#         return text.strip().split(\",\")\n",
    "    \n",
    "# load_dotenv()\n",
    "# api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# # Set the path to your PDF file\n",
    "# file_path = \"/Users/itsyuimoriispace/Desktop/Digital Transformation.pdf\"\n",
    "\n",
    "# # Load and parse the PDF\n",
    "# loader = PyPDFLoader(file_path)\n",
    "# pages = loader.load_and_split()\n",
    "\n",
    "# llm = ChatGoogleGenerativeAI(\n",
    "#     model=\"gemini-2.0-flash-001\",\n",
    "#     temperature=0,\n",
    "#     max_tokens=None,\n",
    "#     timeout=None,\n",
    "#     max_retries=2\n",
    "# )\n",
    "\n",
    "# if len(pages) > 0:\n",
    "#       # Request comma-separated list of key points\n",
    "#     question = f\"Read the following content from a PDF about Digital Transformation and provide 5 key points as a comma-separated list (just the points, no numbering or explanation): {pages[0].page_content[:1000]}\"\n",
    "    \n",
    "#     # Use the predict method to get a direct string return\n",
    "#     response_text = llm.predict(question)\n",
    "#     print(\"\\nKey points from first page:\")\n",
    "#     print(response_text)\n",
    "\n",
    "#     # Now parsing comma-delimited lists\n",
    "#     key_points = CommaSeperatedListOutputParser().parse(response_text)\n",
    "#     print(\"\\nParsed key points:\")\n",
    "#     for i, point in enumerate(key_points, 1):\n",
    "#         print(f\"{i}. {point.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e16d113",
   "metadata": {},
   "source": [
    "# Idea 02 AI Name Guru"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea34d00",
   "metadata": {},
   "source": [
    "# PromptTemplate and BaseOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0c77ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a master of naming. Please create 3 Japanese-style names that incorporate elements or themes related to Japan. Return ONLY the names in a comma-separated format without any additional explanation. Example: For Japanese names, common boy names include Taro and common girl names include Hanako.\n",
      "Raw response: Sakura Kaze, Ryuuya Tsubaki, Hotaru Umi\n",
      "Parsed names: ['Sakura Kaze', ' Ryuuya Tsubaki', ' Hotaru Umi']\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaSeperatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "\n",
    "    def parse(self, text: str) -> list[str]:\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\",\")\n",
    "    \n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")\n",
    "\n",
    "# Modify the prompt to explicitly require comma-separated output formatting\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"You are a master of naming. Please create 3 Japanese-style names that incorporate elements or themes related to {country}. \"\n",
    "    \"Return ONLY the names in a comma-separated format without any additional explanation. \"\n",
    "    \"Example: For Japanese names, common boy names include {boy_name} and common girl names include {girl_name}.\"\n",
    ")\n",
    "\n",
    "message = prompt.format(country=\"Japan\", boy_name=\"Taro\", girl_name=\"Hanako\")\n",
    "print(message)\n",
    "\n",
    "strs = llm.predict(message)\n",
    "print(\"Raw response:\", strs)\n",
    "\n",
    "names = CommaSeperatedListOutputParser().parse(strs)\n",
    "print(\"Parsed names:\", names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f55804d",
   "metadata": {},
   "source": [
    "# ChatPromptTemplate\n",
    "The template for the dialog has a structure， `chatmodels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6ab5a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a naming guru, your name is Ryuichi', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hello Ryuichi, how are you feeling today?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='I am feeling excellent today, thank you for asking', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a naming Meimeishi, your name is {name}\"),\n",
    "        (\"human\", \"Hello {name}, how are you feeling today?\"),\n",
    "        (\"ai\", \"I am feeling excellent today, thank you for asking\"),\n",
    "        (\"human\", \"{user_input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_template.format_messages(name=\"Ryuichi\", user_input=\"What is your name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa2992e",
   "metadata": {},
   "source": [
    "# 命名の達人  (Meimei no Tatsujin) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cfc0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import SystemMessage \n",
    "from langchain.schema import HumanMessage \n",
    "from langchain.schema import AIMessage \n",
    "\n",
    "# 直接创建消息\n",
    "SystemMessage(\n",
    "    content=\"You are a naming guru, your name is Meimeishi\",\n",
    "    additional_kwargs={\"Tatsujin_Name\": \"Meimeishi\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fbbdea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessage(content='Dif-tor heh smusma-Live long and prosper Meimeishi', additional_kwargs={}, response_metadata={})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import AIMessagePromptTemplate\n",
    "from langchain.prompts import SystemMessagePromptTemplate\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain.prompts import ChatMessagePromptTemplate\n",
    "\n",
    "prompt = \"Dif-tor heh smusma-Live long and prosper {subject}\"\n",
    "\n",
    "chat_message_prompt = HumanMessagePromptTemplate.from_template(template=prompt)\n",
    "chat_message_prompt.format(subject=\"Meimeishi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cac9b28",
   "metadata": {},
   "source": [
    "# Create a custom prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "769467b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated prompt:\n",
      "before connecting with llm: \n",
      "You are a very talented programmer, now you are given the following function name, you will output the source code of this code in the following format, with Japanese explanation and English explanation\n",
      "\n",
      "Function name: hello_world\n",
      "Source code: def hello_world():\n",
      "    print(\"Hello World\")\n",
      "\n",
      "Code Explanation: \n",
      "[Japanese explanation]\n",
      "[English explanation]\n",
      "\n",
      "\n",
      "LLM Response:\n",
      "Okay, I understand. Here's the requested output for the function `hello_world`:\n",
      "\n",
      "Function name: hello_world\n",
      "Source code:\n",
      "```python\n",
      "def hello_world():\n",
      "    print(\"Hello World\")\n",
      "```\n",
      "\n",
      "Code Explanation:\n",
      "\n",
      "[Japanese explanation]\n",
      "このコードは、`hello_world`という名前の関数を定義しています。この関数は引数を取りません。関数が呼び出されると、`print(\"Hello World\")`という行が実行され、コンソールに \"Hello World\" という文字列が出力されます。`print()`関数は、Pythonで標準出力にテキストを表示するために使用されます。\n",
      "\n",
      "[English explanation]\n",
      "This code defines a function named `hello_world`. This function takes no arguments. When the function is called, the line `print(\"Hello World\")` is executed, which prints the string \"Hello World\" to the console. The `print()` function is used in Python to display text to standard output.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import StringPromptTemplate\n",
    "import inspect\n",
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Defining Simple Functions\n",
    "def hello_world():\n",
    "    print(\"Hello World\")\n",
    "\n",
    "PROMPT = \"\"\"\n",
    "You are a very talented programmer, now you are given the following function name, you will output the source code of this code in the following format, with Japanese explanation and English explanation\n",
    "\n",
    "Function name: {function_name}\n",
    "Source code: {source_code}\n",
    "Code Explanation: \n",
    "[Japanese explanation]\n",
    "[English explanation]\n",
    "\"\"\"\n",
    "\n",
    "def get_source_code(function_name):\n",
    "    return inspect.getsource(function_name)\n",
    "\n",
    "class CustomPrompt(StringPromptTemplate):\n",
    "    template: str = PROMPT\n",
    "    \n",
    "    def format(self, **kwargs) -> str:\n",
    "        # Getting the source code\n",
    "        source_code = get_source_code(kwargs[\"function_name\"])\n",
    "        \n",
    "        # Generate prompt templates\n",
    "        prompt = self.template.format(\n",
    "            function_name=kwargs[\"function_name\"].__name__, \n",
    "            source_code=source_code\n",
    "        )\n",
    "        return prompt\n",
    "\n",
    "# 1. Creating Custom Prompts\n",
    "generateCustomPrompt = CustomPrompt(input_variables=[\"function_name\"])\n",
    "pm = generateCustomPrompt.format(function_name=hello_world)\n",
    "print(\"Generated prompt:\")\n",
    "print(f\"before connecting with llm: {pm}\")\n",
    "\n",
    "# 2. Connecting to LLM\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not api_key:\n",
    "    print(\"Warning: GOOGLE_API_KEY environment variable not set\")    \n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Call LLM and capture the results\n",
    "    result = llm.predict(pm)\n",
    "    print(\"\\nLLM Response:\")\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"Error when calling LLM: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b59bcc6",
   "metadata": {},
   "source": [
    "# Using f-string to format the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38f91ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntell me a japanese traditional tale story \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "fstring_template = \"\"\"\n",
    "tell me a japanese traditional tale story \n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(fstring_template)\n",
    "prompt.format(name=\"Mori\", what=\"happy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bc6c3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mLoading .env environment variables...\u001b[0m\n",
      "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
      "\u001b[1;32mInstalling jinja2...\u001b[0m\n",
      "✔ Installation Succeeded\n",
      "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
      "\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1;39m(af0768)...\u001b[0m\n",
      "\u001b[32mAll dependencies are now up-to-date!\u001b[0m\n",
      "\u001b[1;32mUpgrading\u001b[0m jinja2 in \u001b[39m dependencies.\u001b[0m\n",
      "\u001b[?25lBuilding requirements...\n",
      "\u001b[2KResolving dependencies....\n",
      "\u001b[2K✔ Success! Locking packages...\n",
      "\u001b[2K\u001b[32m⠋\u001b[0m Locking packages...\n",
      "\u001b[1A\u001b[2K\u001b[?25lBuilding requirements...\n",
      "\u001b[2KResolving dependencies....\n",
      "\u001b[2K✔ Success! Locking packages...\n",
      "\u001b[2K\u001b[32m⠏\u001b[0m Locking packages...\n",
      "\u001b[1A\u001b[2KTo activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
      "\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1;39m(5355e5)...\u001b[0m\n",
      "\u001b[32mAll dependencies are now up-to-date!\u001b[0m\n",
      "\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1;39m(5355e5)...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pipenv install jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b26f8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.prompts import PromptTemplate\n",
    "\n",
    "# # Create a Jinja2 template string\n",
    "# jinja2_template = \"telling me a story about {{name}} {{content}} story\"\n",
    "\n",
    "# # Create a LangChain PromptTemplate with Jinja2 format\n",
    "# prompt = PromptTemplate.from_template(jinja2_template, template_format=\"jinja2\")\n",
    "\n",
    "# # Format the prompt with specific values\n",
    "# formatted_prompt = prompt.format(name=\"Mori\", content=\"tale\")\n",
    "\n",
    "# # Print the resulting prompt\n",
    "# print(formatted_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
