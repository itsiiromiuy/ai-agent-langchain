{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c2cd5e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c00fda",
   "metadata": {},
   "source": [
    "# Re-install the environment\n",
    "deactivate\n",
    "rm -rf venv\n",
    "\n",
    "/opt/homebrew/bin/python3.10 -m venv venv\n",
    "\n",
    "source venv/bin/activate\n",
    "python --version\n",
    "pip install --upgrade pip\n",
    "pip install langchain google-generativeai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76582aaa",
   "metadata": {},
   "source": [
    "Environment setup \n",
    "- Install local `pyenv` for local environment management.\n",
    "- Set the local directory to the desired python version.\n",
    "- Install the `Langchain` package.\n",
    "- Start the virtual environment `source ~/Documents/github/ai-agent/venv/bin/activate`.\n",
    "- Use `pip install ipykernel`.  \n",
    "    - `ipykerne`l` is Jupyter's tool for connecting to specific Python environments.\n",
    "    - It allows `Jupyter Notebook` to use the Python interpreters and libraries in the virtual environment.\n",
    "- `jupyter kernelspec list` Verify that the kernel is installed:\n",
    "\n",
    "\n",
    "環境設定 \n",
    "- ローカル環境を管理するために、ローカルの `pyenv` をインストールする。\n",
    "- ローカルのディレクトリを目的の python のバージョンに設定する。\n",
    "- Langchain` パッケージをインストールする。\n",
    "- 仮想環境 `source ~/Documents/github/ai-agent/venv/bin/activate` を起動する。\n",
    "- pip install ipykernel` を使用する。 \n",
    "    - ipykerne`l` は特定のPython環境に接続するためのJupyterのツールである。\n",
    "    - これによって `Jupyter Notebook` は仮想環境のPythonインタプリタとライブラリを利用できるようになる。\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ab5c09",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# pyenv versions\n",
    "# pyenv shell 3.10\n",
    "# python -m venv .venv\n",
    "# source .venv/bin/activate\n",
    "# pip install google-generativeai\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "654baa96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5781.42s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-ai-generativelanguage==0.6.17 in ./new_venv/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (0.6.17)\n",
      "Requirement already satisfied: dotenv==0.9.9 in ./new_venv/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (0.9.9)\n",
      "Requirement already satisfied: langchain-google-genai==2.1.2 in ./new_venv/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (2.1.2)\n",
      "Collecting langchain-community==0.3.21 (from -r requirements.txt (line 4))\n",
      "  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in ./new_venv/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage==0.6.17->-r requirements.txt (line 1)) (2.24.2)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in ./new_venv/lib/python3.10/site-packages (from google-ai-generativelanguage==0.6.17->-r requirements.txt (line 1)) (2.38.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in ./new_venv/lib/python3.10/site-packages (from google-ai-generativelanguage==0.6.17->-r requirements.txt (line 1)) (1.26.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in ./new_venv/lib/python3.10/site-packages (from google-ai-generativelanguage==0.6.17->-r requirements.txt (line 1)) (5.29.4)\n",
      "Requirement already satisfied: python-dotenv in ./new_venv/lib/python3.10/site-packages (from dotenv==0.9.9->-r requirements.txt (line 2)) (1.1.0)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in ./new_venv/lib/python3.10/site-packages (from langchain-google-genai==2.1.2->-r requirements.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.49 in ./new_venv/lib/python3.10/site-packages (from langchain-google-genai==2.1.2->-r requirements.txt (line 3)) (0.3.51)\n",
      "Requirement already satisfied: pydantic<3,>=2 in ./new_venv/lib/python3.10/site-packages (from langchain-google-genai==2.1.2->-r requirements.txt (line 3)) (2.11.3)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.23 in ./new_venv/lib/python3.10/site-packages (from langchain-community==0.3.21->-r requirements.txt (line 4)) (0.3.23)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./new_venv/lib/python3.10/site-packages (from langchain-community==0.3.21->-r requirements.txt (line 4)) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in ./new_venv/lib/python3.10/site-packages (from langchain-community==0.3.21->-r requirements.txt (line 4)) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./new_venv/lib/python3.10/site-packages (from langchain-community==0.3.21->-r requirements.txt (line 4)) (6.0.2)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community==0.3.21->-r requirements.txt (line 4))\n",
      "  Downloading aiohttp-3.11.16-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in ./new_venv/lib/python3.10/site-packages (from langchain-community==0.3.21->-r requirements.txt (line 4)) (9.1.2)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community==0.3.21->-r requirements.txt (line 4))\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community==0.3.21->-r requirements.txt (line 4))\n",
      "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in ./new_venv/lib/python3.10/site-packages (from langchain-community==0.3.21->-r requirements.txt (line 4)) (0.3.28)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community==0.3.21->-r requirements.txt (line 4))\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting numpy<3,>=1.26.2 (from langchain-community==0.3.21->-r requirements.txt (line 4))\n",
      "  Downloading numpy-2.2.4-cp310-cp310-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.21->-r requirements.txt (line 4))\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.21->-r requirements.txt (line 4))\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./new_venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.21->-r requirements.txt (line 4)) (4.0.3)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.21->-r requirements.txt (line 4))\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.21->-r requirements.txt (line 4))\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.21->-r requirements.txt (line 4))\n",
      "  Downloading multidict-6.4.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.21->-r requirements.txt (line 4))\n",
      "  Downloading propcache-0.3.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.21->-r requirements.txt (line 4))\n",
      "  Downloading yarl-1.19.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (71 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.21->-r requirements.txt (line 4))\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.21->-r requirements.txt (line 4))\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in ./new_venv/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage==0.6.17->-r requirements.txt (line 1)) (1.69.2)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in ./new_venv/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage==0.6.17->-r requirements.txt (line 1)) (1.72.0rc1)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in ./new_venv/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage==0.6.17->-r requirements.txt (line 1)) (1.71.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./new_venv/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage==0.6.17->-r requirements.txt (line 1)) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./new_venv/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage==0.6.17->-r requirements.txt (line 1)) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./new_venv/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage==0.6.17->-r requirements.txt (line 1)) (4.9)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in ./new_venv/lib/python3.10/site-packages (from langchain<1.0.0,>=0.3.23->langchain-community==0.3.21->-r requirements.txt (line 4)) (0.3.8)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./new_venv/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.49->langchain-google-genai==2.1.2->-r requirements.txt (line 3)) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./new_venv/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.49->langchain-google-genai==2.1.2->-r requirements.txt (line 3)) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./new_venv/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.49->langchain-google-genai==2.1.2->-r requirements.txt (line 3)) (4.13.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./new_venv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-community==0.3.21->-r requirements.txt (line 4)) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./new_venv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-community==0.3.21->-r requirements.txt (line 4)) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./new_venv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-community==0.3.21->-r requirements.txt (line 4)) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./new_venv/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-community==0.3.21->-r requirements.txt (line 4)) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./new_venv/lib/python3.10/site-packages (from pydantic<3,>=2->langchain-google-genai==2.1.2->-r requirements.txt (line 3)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in ./new_venv/lib/python3.10/site-packages (from pydantic<3,>=2->langchain-google-genai==2.1.2->-r requirements.txt (line 3)) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./new_venv/lib/python3.10/site-packages (from pydantic<3,>=2->langchain-google-genai==2.1.2->-r requirements.txt (line 3)) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./new_venv/lib/python3.10/site-packages (from requests<3,>=2->langchain-community==0.3.21->-r requirements.txt (line 4)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./new_venv/lib/python3.10/site-packages (from requests<3,>=2->langchain-community==0.3.21->-r requirements.txt (line 4)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./new_venv/lib/python3.10/site-packages (from requests<3,>=2->langchain-community==0.3.21->-r requirements.txt (line 4)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./new_venv/lib/python3.10/site-packages (from requests<3,>=2->langchain-community==0.3.21->-r requirements.txt (line 4)) (2025.1.31)\n",
      "Requirement already satisfied: anyio in ./new_venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community==0.3.21->-r requirements.txt (line 4)) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./new_venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community==0.3.21->-r requirements.txt (line 4)) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./new_venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community==0.3.21->-r requirements.txt (line 4)) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./new_venv/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.49->langchain-google-genai==2.1.2->-r requirements.txt (line 3)) (3.0.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in ./new_venv/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage==0.6.17->-r requirements.txt (line 1)) (0.6.1)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.21->-r requirements.txt (line 4))\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./new_venv/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community==0.3.21->-r requirements.txt (line 4)) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./new_venv/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community==0.3.21->-r requirements.txt (line 4)) (1.3.1)\n",
      "Downloading langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m224.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.11.16-cp310-cp310-macosx_11_0_arm64.whl (455 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading numpy-2.2.4-cp310-cp310-macosx_14_0_arm64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m207.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.5.0-cp310-cp310-macosx_11_0_arm64.whl (52 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading multidict-6.4.2-cp310-cp310-macosx_11_0_arm64.whl (37 kB)\n",
      "Downloading propcache-0.3.1-cp310-cp310-macosx_11_0_arm64.whl (45 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading yarl-1.19.0-cp310-cp310-macosx_11_0_arm64.whl (94 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: propcache, numpy, mypy-extensions, multidict, marshmallow, httpx-sse, frozenlist, attrs, aiohappyeyeballs, yarl, typing-inspect, aiosignal, pydantic-settings, dataclasses-json, aiohttp, langchain-community\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.16 aiosignal-1.3.2 attrs-25.3.0 dataclasses-json-0.6.7 frozenlist-1.5.0 httpx-sse-0.4.0 langchain-community-0.3.21 marshmallow-3.26.1 multidict-6.4.2 mypy-extensions-1.0.0 numpy-2.2.4 propcache-0.3.1 pydantic-settings-2.8.1 typing-inspect-0.9.0 yarl-1.19.0\n"
     ]
    }
   ],
   "source": [
    "#! pip install --upgrade langchain -i https://pypi.org/simple\n",
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e42457",
   "metadata": {},
   "source": [
    "# Langchain example\n",
    "1. create LLM\n",
    "2. Custom Prompt Template \n",
    "- Context Templating Approach:\n",
    "- Support dynamic input parameters:\n",
    "    - Subject/topic\n",
    "    - Difficulty level\n",
    "    - Number of questions\n",
    "    - Question type (multiple choice, true/false, etc.)\n",
    "3. Output Interpreter\n",
    "- Advanced Parsing Capabilities:\n",
    "    - Convert LLM output to structured formats\n",
    "    - Support for multiple output types:\n",
    "        - JSON\n",
    "        - Dictionary\n",
    "        - Structured list\n",
    "- Implement error handling and validation\n",
    "4. First Implementation\n",
    "- Interactive Quiz Generation\n",
    "- Dynamic Parameters:\n",
    "    - Subject selection\n",
    "    - Difficulty level\n",
    "    - Number of questions\n",
    "- Flexible Input Handling\n",
    "- Structured Output Generation\n",
    "\n",
    "- `LLM.predict()` predict(): Generates next tokens\n",
    "\n",
    "- `langchain.prompts`\n",
    "- `prompt.format(category=\"\")`\n",
    "- `langchain.schema` \n",
    "- `OutputParser.parse()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dbb95413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-2.5-pro-exp-03-25\n",
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/learnlm-1.5-pro-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# 列出可用模型\n",
    "for m in genai.list_models():\n",
    "    if 'generateContent' in m.supported_generation_methods:\n",
    "        print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3f0627eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/itsyuimoriispace/Documents/github/ai-agent/new_venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "# export GOOGLE_API_KEY=AIzaSyBx6_WRVw51fTN5TBwfzaFkpXqh0LOr1us\n",
    "\n",
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0a30b30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The coder toiled, in screens\\' pale light,\\nWith APIs a tangled, messy sight.\\nTo weave a chain of thought, a flowing stream,\\nA digital bard, a waking dream.\\n\\nThen whispers spread, a name took hold,\\nOf LangChain forged, in Python bold.\\nA framework strong, a guiding hand,\\nTo link the models, \\'cross the land.\\n\\nFrom LLMs grand, to chatbots small,\\nIt bound them close, to heed the call.\\nTo question deep, to summarize,\\nAnd from the data, insights rise.\\n\\nThe coder wept, with joy profound,\\nNo more lost in code, unbound.\\nWith chains of agents, linked with grace,\\nComplex tasks performed, in time and space.\\n\\nHe built a bot, to write a tale,\\nOf knights and dragons, in a moonlit vale.\\nHe crafted tools, for question\\'s quest,\\nTo find the answer, truly best.\\n\\nBut shadows lurked, in code\\'s dark heart,\\nA prompt injection, tore it apart.\\nThe chain grew wild, the output strange,\\nA digital beast, beyond the range.\\n\\nThe coder fought, with valiant key,\\nTo tame the chain, and set it free.\\nWith careful checks, and filters strong,\\nHe brought the wayward links along.\\n\\nSo learn this tale, of LangChain\\'s might,\\nA tool of power, shining bright.\\nBut wield it well, with cautious hand,\\nFor chaos sleeps, in this coded land.\\n\\nAnd if you hear, a whispered plea,\\nFrom models deep, \"Set us free!\"\\nRemember then, the coder\\'s plight,\\nAnd keep your chains, both strong and tight.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-pro-002', 'safety_ratings': []} id='run-1fd7dbfa-cb99-4ffd-b29c-729409f09f3a-0' usage_metadata={'input_tokens': 7, 'output_tokens': 363, 'total_tokens': 370, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # This loads variables from .env file\n",
    "\n",
    "# Now use LangChain as normal - it will find the key in environment\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "response = llm.invoke(\"Write me a ballad about LangChain\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2a5a2989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! I am a large language model, trained by Google. I am designed to provide information and complete tasks based on the prompts and questions I receive. I can generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way. \\n\\nThink of me as a helpful and knowledgeable assistant, ready to assist you with a wide range of topics. How can I help you today?'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(  model=\"gemini-2.0-flash-001\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")\n",
    "llm.predict(\"Introduce yourself\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb48e4b",
   "metadata": {},
   "source": [
    "# Using Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9e594a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a PDF reader, read the following PDF and answer the question. Hello, how are you?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Please provide me with the PDF file. I need the content of the PDF to be able to read it and answer your question. I cannot access files from your computer or the internet without you providing them to me.\\n\\nOnce you provide the PDF content, I will do my best to read it and answer your question.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(  model=\"gemini-2.0-flash-001\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")\n",
    "\n",
    "prompt= PromptTemplate.from_template(\"You are a PDF reader, read the following PDF and answer the question. {pdf_text}\")\n",
    "message = prompt.format(pdf_text=\"Hello, how are you?\")\n",
    "print(message)\n",
    "llm.predict(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0222677",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6993fc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/itsyuimoriispace/Documents/github/ai-agent/new_venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2797b4be",
   "metadata": {},
   "source": [
    "# PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cce7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 pages from the PDF\n",
      "\n",
      "Summary of first page:\n",
      "content=\"This document outlines the required information for applicants to the Digital Transformation and Innovation (DTI) Master's program. Applicants need to provide their first name, last name, student number, and email address. They must also specify their major field of study in their Bachelor's program and any graduate degrees they hold. Furthermore, the document requires applicants to list specific courses they've taken, including one in Statistics, one in Computing, and two advanced courses demonstrating specialization in either creative arts and humanities, management, or technology, along with the course code, title, year, and grade for each.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash-001', 'safety_ratings': []} id='run-f38279b7-c187-4c18-b7e2-93754c634ed6-0' usage_metadata={'input_tokens': 297, 'output_tokens': 121, 'total_tokens': 418, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    " \n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import sys\n",
    "sys.path.append('/opt/homebrew/lib/python3.10/site-packages')\n",
    "\n",
    "# Now your imports should work\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "# Load environment variables (including your Google API key)\n",
    "load_dotenv()\n",
    "\n",
    "# Set the path to your PDF file\n",
    "file_path = \"/Users/itsyuimoriispace/Desktop/Digital Transformation.pdf\"\n",
    "\n",
    "# Load and parse the PDF\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# Print the number of pages found\n",
    "print(f\"Loaded {len(pages)} pages from the PDF\")\n",
    "\n",
    "# Initialize the Gemini model\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    temperature=0,\n",
    "    max_tokens=None\n",
    ")\n",
    "\n",
    "# If you want to analyze specific content from the PDF\n",
    "if len(pages) > 0:\n",
    "    # Example: Summarize the first page\n",
    "    question = f\"Summarize the following content from a PDF about Digital Transformation: {pages[0].page_content[:1000]}\"\n",
    "    response = llm.invoke(question)\n",
    "    print(\"\\nSummary of first page:\")\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8e5f27",
   "metadata": {},
   "source": [
    "# Formatting output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "115b6648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ' how are you?']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "# Self-defining class, inherits BaseOutputParser.\n",
    "class CommaSeperatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "\n",
    "    def parse(self, text: str) -> list[str]:\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\",\")\n",
    "    \n",
    "CommaSeperatedListOutputParser().parse(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4551cfb",
   "metadata": {},
   "source": [
    "#  Idea01: PDF processing and a large language model such as Gemini to create a chatbox\n",
    "Using LangChain in conjunction with PDF processing and a large language model such as Gemini to create a chatbox that can talk to PDF documents is a great direction to take. You've got a good starting point, so let me explain further how to accomplish this.\n",
    "To create a chatbox that can talk to PDFs, you need to add the following key components:\n",
    "\n",
    "Vector storage: Convert PDF content into vectors and store them so that semantic search is possible\n",
    "Retrieval system: retrieve relevant content based on user questions\n",
    "Conversation Memory: Keeps conversations in context\n",
    "Conversation Chaining: Connects all components into a complete conversation system.\n",
    "\n",
    "Translated with www.DeepL.com/Translator (free version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3b8fa0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Key points from first page:\n",
      "Applicant Identification, Major field of study, Graduate degree specification, Email, Required Courses\n",
      "\n",
      "Parsed key points:\n",
      "1. Applicant Identification\n",
      "2. Major field of study\n",
      "3. Graduate degree specification\n",
      "4. Email\n",
      "5. Required Courses\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaSeperatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "\n",
    "    def parse(self, text: str) -> list[str]:\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\",\")\n",
    "    \n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Set the path to your PDF file\n",
    "file_path = \"/Users/itsyuimoriispace/Desktop/Digital Transformation.pdf\"\n",
    "\n",
    "# Load and parse the PDF\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")\n",
    "\n",
    "if len(pages) > 0:\n",
    "      # Request comma-separated list of key points\n",
    "    question = f\"Read the following content from a PDF about Digital Transformation and provide 5 key points as a comma-separated list (just the points, no numbering or explanation): {pages[0].page_content[:1000]}\"\n",
    "    \n",
    "    # Use the predict method to get a direct string return\n",
    "    response_text = llm.predict(question)\n",
    "    print(\"\\nKey points from first page:\")\n",
    "    print(response_text)\n",
    "\n",
    "    # Now parsing comma-delimited lists\n",
    "    key_points = CommaSeperatedListOutputParser().parse(response_text)\n",
    "    print(\"\\nParsed key points:\")\n",
    "    for i, point in enumerate(key_points, 1):\n",
    "        print(f\"{i}. {point.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e16d113",
   "metadata": {},
   "source": [
    "# Idea 02 AI Name Guru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0c77ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a master of naming. Please create 3 Japanese-style names that incorporate elements or themes related to Japan. Return ONLY the names in a comma-separated format without any additional explanation. Example: For Japanese names, common boy names include Taro and common girl names include Hanako.\n",
      "Raw response: Sakura Kaze, Ryuuya Tsubaki, Hotaru Umi\n",
      "Parsed names: ['Sakura Kaze', ' Ryuuya Tsubaki', ' Hotaru Umi']\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaSeperatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "\n",
    "    def parse(self, text: str) -> list[str]:\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\",\")\n",
    "    \n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")\n",
    "\n",
    "# Modify the prompt to explicitly require comma-separated output formatting\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"You are a master of naming. Please create 3 Japanese-style names that incorporate elements or themes related to {country}. \"\n",
    "    \"Return ONLY the names in a comma-separated format without any additional explanation. \"\n",
    "    \"Example: For Japanese names, common boy names include {boy_name} and common girl names include {girl_name}.\"\n",
    ")\n",
    "\n",
    "message = prompt.format(country=\"Japan\", boy_name=\"Taro\", girl_name=\"Hanako\")\n",
    "print(message)\n",
    "\n",
    "strs = llm.predict(message)\n",
    "print(\"Raw response:\", strs)\n",
    "\n",
    "names = CommaSeperatedListOutputParser().parse(strs)\n",
    "print(\"Parsed names:\", names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
