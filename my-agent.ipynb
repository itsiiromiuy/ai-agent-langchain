{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c2cd5e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c00fda",
   "metadata": {},
   "source": [
    "# Re-install the environment\n",
    "deactivate\n",
    "rm -rf venv\n",
    "\n",
    "/opt/homebrew/bin/python3.10 -m venv venv\n",
    "\n",
    "source venv/bin/activate\n",
    "python --version\n",
    "pip install --upgrade pip\n",
    "pip install langchain google-generativeai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76582aaa",
   "metadata": {},
   "source": [
    "Environment setup \n",
    "- Install local `pyenv` for local environment management.\n",
    "- Set the local directory to the desired python version.\n",
    "- Install the `Langchain` package.\n",
    "- Start the virtual environment `source ~/Documents/github/ai-agent/venv/bin/activate`.\n",
    "- Use `pip install ipykernel`.  \n",
    "    - `ipykerne`l` is Jupyter's tool for connecting to specific Python environments.\n",
    "    - It allows `Jupyter Notebook` to use the Python interpreters and libraries in the virtual environment.\n",
    "- `jupyter kernelspec list` Verify that the kernel is installed:\n",
    "\n",
    "\n",
    "環境設定 \n",
    "- ローカル環境を管理するために、ローカルの `pyenv` をインストールする。\n",
    "- ローカルのディレクトリを目的の python のバージョンに設定する。\n",
    "- Langchain` パッケージをインストールする。\n",
    "- 仮想環境 `source ~/Documents/github/ai-agent/venv/bin/activate` を起動する。\n",
    "- pip install ipykernel` を使用する。 \n",
    "    - ipykerne`l` は特定のPython環境に接続するためのJupyterのツールである。\n",
    "    - これによって `Jupyter Notebook` は仮想環境のPythonインタプリタとライブラリを利用できるようになる。\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "654baa96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: /Users/itsyuimorii/Documents/github-new/ai-agent-langchain/new_venv/bin/pip: bad interpreter: /Users/itsyuimoriispace/Documents/github/ai-agent/new_venv/bin/python3.10: no such file or directory\n"
     ]
    }
   ],
   "source": [
    "#! pip install --upgrade langchain -i https://pypi.org/simple\n",
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e42457",
   "metadata": {},
   "source": [
    "# Langchain example\n",
    "1. create LLM\n",
    "2. Custom Prompt Template \n",
    "- Context Templating Approach:\n",
    "- Support dynamic input parameters:\n",
    "    - Subject/topic\n",
    "    - Difficulty level\n",
    "    - Number of questions\n",
    "    - Question type (multiple choice, true/false, etc.)\n",
    "3. Output Interpreter\n",
    "- Advanced Parsing Capabilities:\n",
    "    - Convert LLM output to structured formats\n",
    "    - Support for multiple output types:\n",
    "        - JSON\n",
    "        - Dictionary\n",
    "        - Structured list\n",
    "- Implement error handling and validation\n",
    "4. First Implementation\n",
    "- Interactive Quiz Generation\n",
    "- Dynamic Parameters:\n",
    "    - Subject selection\n",
    "    - Difficulty level\n",
    "    - Number of questions\n",
    "- Flexible Input Handling\n",
    "- Structured Output Generation\n",
    "\n",
    "- `LLM.predict()` predict(): Generates next tokens\n",
    "\n",
    "- `langchain.prompts`\n",
    "- `prompt.format(category=\"\")`\n",
    "- `langchain.schema` \n",
    "- `OutputParser.parse()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dbb95413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-2.5-pro-exp-03-25\n",
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/learnlm-1.5-pro-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# 列出可用模型\n",
    "for m in genai.list_models():\n",
    "    if 'generateContent' in m.supported_generation_methods:\n",
    "        print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3f0627eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/itsyuimorii/Documents/github-new/ai-agent-langchain/new_venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "# export GOOGLE_API_KEY=AIzaSyBx6_WRVw51fTN5TBwfzaFkpXqh0LOr1us\n",
    "\n",
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0a30b30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"The coder toiled, in screens' pale light,\\nWith APIs a-scattered, day and night.\\nA thousand calls, a million lines,\\nTo weave a web, of grand designs.\\nBut fractured tools, and broken chains,\\nLeft dreams adrift, in coding rains.\\n\\nThen whispers rose, upon the breeze,\\nOf LangChain's grace, and promised ease.\\nA framework forged, in Python's fire,\\nTo bind the links, and raise them higher.\\nLLMs arose, with voices clear,\\nTo banish bugs, and quell all fear.\\n\\nFrom OpenAI's depths, a model grand,\\nTo Hugging Face, a helping hand.\\nWith agents swift, and memories deep,\\nLangChain's embrace, the secrets keep.\\nNo more the toil, of endless quest,\\nFor scattered tools, and fragmented rest.\\n\\nThe coder smiled, a weary soul,\\nAs chains of logic, took control.\\nA chatbot born, with wit and grace,\\nA question asked, in time and space.\\nFrom documents vast, and knowledge deep,\\nThe answers flowed, while others sleep.\\n\\nThe whispers grew, to shouts of praise,\\nFor LangChain's power, in countless ways.\\nFrom simple tasks, to complex schemes,\\nIt wove the threads, of coded dreams.\\nSo raise a glass, to this new age,\\nWhere language reigns, upon the stage.\\n\\nAnd if you find, your code astray,\\nLet LangChain guide you, on your way.\\nFor in its strength, a hope resides,\\nTo build the future, side by side.\\nSo let the chains, connect and bind,\\nAnd leave the scattered past, behind.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-pro-002', 'safety_ratings': []} id='run-6dace721-ea67-41af-8fe7-b32fbc7e45c4-0' usage_metadata={'input_tokens': 7, 'output_tokens': 362, 'total_tokens': 369, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # This loads variables from .env file\n",
    "\n",
    "# Now use LangChain as normal - it will find the key in environment\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "response = llm.invoke(\"Write me a ballad about LangChain\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2a5a2989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! I am a large language model, trained by Google. I am designed to provide information and complete tasks based on the prompts and questions I receive. I can generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way. \\n\\nThink of me as a helpful and knowledgeable assistant, ready to assist you with a wide range of topics. How can I help you today?'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(  model=\"gemini-2.0-flash-001\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")\n",
    "llm.predict(\"Introduce yourself\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb48e4b",
   "metadata": {},
   "source": [
    "# Using `LangChain` Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9e594a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a PDF reader, read the following PDF and answer the question. Hello, how are you?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Please provide me with the PDF content. I need the text of the PDF to be able to read it and answer your question. I can't access files directly.\\n\\nOnce you provide the text, I will be happy to help!\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(  model=\"gemini-2.0-flash-001\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")\n",
    "\n",
    "prompt= PromptTemplate.from_template(\"You are a PDF reader, read the following PDF and answer the question. {pdf_text}\")\n",
    "message = prompt.format(pdf_text=\"Hello, how are you?\")\n",
    "print(message)\n",
    "llm.predict(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0222677",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6993fc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/itsyuimorii/Documents/github-new/ai-agent-langchain/new_venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2797b4be",
   "metadata": {},
   "source": [
    "# PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "36cce7e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File path /Users/itsyuimoriispace/Desktop/Digital Transformation.pdf is not a valid file or url",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/itsyuimoriispace/Desktop/Digital Transformation.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Load and parse the PDF\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m loader \u001b[38;5;241m=\u001b[39m \u001b[43mPyPDFLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m pages \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload_and_split()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Print the number of pages found\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/github-new/ai-agent-langchain/new_venv/lib/python3.10/site-packages/langchain_community/document_loaders/pdf.py:281\u001b[0m, in \u001b[0;36mPyPDFLoader.__init__\u001b[0;34m(self, file_path, password, headers, extract_images, mode, images_parser, images_inner_format, pages_delimiter, extraction_mode, extraction_kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    240\u001b[0m     file_path: Union[\u001b[38;5;28mstr\u001b[39m, PurePath],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m     extraction_kwargs: Optional[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    251\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize with a file path.\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;03m        `aload` methods to retrieve parsed documents with content and metadata.\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser \u001b[38;5;241m=\u001b[39m PyPDFParser(\n\u001b[1;32m    283\u001b[0m         password\u001b[38;5;241m=\u001b[39mpassword,\n\u001b[1;32m    284\u001b[0m         mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    290\u001b[0m         extraction_kwargs\u001b[38;5;241m=\u001b[39mextraction_kwargs,\n\u001b[1;32m    291\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/github-new/ai-agent-langchain/new_venv/lib/python3.10/site-packages/langchain_community/document_loaders/pdf.py:140\u001b[0m, in \u001b[0;36mBasePDFLoader.__init__\u001b[0;34m(self, file_path, headers)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(temp_pdf)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path):\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile path \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not a valid file or url\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path)\n",
      "\u001b[0;31mValueError\u001b[0m: File path /Users/itsyuimoriispace/Desktop/Digital Transformation.pdf is not a valid file or url"
     ]
    }
   ],
   "source": [
    " \n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import sys\n",
    "sys.path.append('/opt/homebrew/lib/python3.10/site-packages')\n",
    "\n",
    "# Now your imports should work\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "# Load environment variables (including your Google API key)\n",
    "load_dotenv()\n",
    "\n",
    "# Set the path to your PDF file\n",
    "file_path = \"/Users/itsyuimoriispace/Desktop/Digital Transformation.pdf\"\n",
    "\n",
    "# Load and parse the PDF\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# Print the number of pages found\n",
    "print(f\"Loaded {len(pages)} pages from the PDF\")\n",
    "\n",
    "# Initialize the Gemini model\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    temperature=0,\n",
    "    max_tokens=None\n",
    ")\n",
    "\n",
    "# If you want to analyze specific content from the PDF\n",
    "if len(pages) > 0:\n",
    "    # Example: Summarize the first page\n",
    "    question = f\"Summarize the following content from a PDF about Digital Transformation: {pages[0].page_content[:1000]}\"\n",
    "    response = llm.invoke(question)\n",
    "    print(\"\\nSummary of first page:\")\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8e5f27",
   "metadata": {},
   "source": [
    "# Formatting output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115b6648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ' how are you?']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "# Self-defining class, inherits BaseOutputParser.\n",
    "class CommaSeperatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "\n",
    "    def parse(self, text: str) -> list[str]:\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\",\")\n",
    "    \n",
    "CommaSeperatedListOutputParser().parse(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4551cfb",
   "metadata": {},
   "source": [
    "#  Idea01: PDF processing and Gemini to create a chatbox\n",
    "Using LangChain in conjunction with PDF processing and a large language model such as Gemini to create a chatbox that can talk to PDF documents is a great direction to take. You've got a good starting point, so let me explain further how to accomplish this.\n",
    "To create a chatbox that can talk to PDFs, you need to add the following key components:\n",
    "\n",
    "Vector storage: Convert PDF content into vectors and store them so that semantic search is possible\n",
    "Retrieval system: retrieve relevant content based on user questions\n",
    "Conversation Memory: Keeps conversations in context\n",
    "Conversation Chaining: Connects all components into a complete conversation system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8fa0df",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File path /Users/itsyuimoriispace/Desktop/Digital Transformation.pdf is not a valid file or url",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/itsyuimoriispace/Desktop/Digital Transformation.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Load and parse the PDF\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m loader \u001b[38;5;241m=\u001b[39m \u001b[43mPyPDFLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m pages \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload_and_split()\n\u001b[1;32m     25\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatGoogleGenerativeAI(\n\u001b[1;32m     26\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-2.0-flash-001\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     max_retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     31\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/github-new/ai-agent-langchain/new_venv/lib/python3.10/site-packages/langchain_community/document_loaders/pdf.py:281\u001b[0m, in \u001b[0;36mPyPDFLoader.__init__\u001b[0;34m(self, file_path, password, headers, extract_images, mode, images_parser, images_inner_format, pages_delimiter, extraction_mode, extraction_kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    240\u001b[0m     file_path: Union[\u001b[38;5;28mstr\u001b[39m, PurePath],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m     extraction_kwargs: Optional[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    251\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize with a file path.\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;03m        `aload` methods to retrieve parsed documents with content and metadata.\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser \u001b[38;5;241m=\u001b[39m PyPDFParser(\n\u001b[1;32m    283\u001b[0m         password\u001b[38;5;241m=\u001b[39mpassword,\n\u001b[1;32m    284\u001b[0m         mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    290\u001b[0m         extraction_kwargs\u001b[38;5;241m=\u001b[39mextraction_kwargs,\n\u001b[1;32m    291\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/github-new/ai-agent-langchain/new_venv/lib/python3.10/site-packages/langchain_community/document_loaders/pdf.py:140\u001b[0m, in \u001b[0;36mBasePDFLoader.__init__\u001b[0;34m(self, file_path, headers)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(temp_pdf)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path):\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile path \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not a valid file or url\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path)\n",
      "\u001b[0;31mValueError\u001b[0m: File path /Users/itsyuimoriispace/Desktop/Digital Transformation.pdf is not a valid file or url"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaSeperatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "\n",
    "    def parse(self, text: str) -> list[str]:\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\",\")\n",
    "    \n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Set the path to your PDF file\n",
    "file_path = \"/Users/itsyuimoriispace/Desktop/Digital Transformation.pdf\"\n",
    "\n",
    "# Load and parse the PDF\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")\n",
    "\n",
    "if len(pages) > 0:\n",
    "      # Request comma-separated list of key points\n",
    "    question = f\"Read the following content from a PDF about Digital Transformation and provide 5 key points as a comma-separated list (just the points, no numbering or explanation): {pages[0].page_content[:1000]}\"\n",
    "    \n",
    "    # Use the predict method to get a direct string return\n",
    "    response_text = llm.predict(question)\n",
    "    print(\"\\nKey points from first page:\")\n",
    "    print(response_text)\n",
    "\n",
    "    # Now parsing comma-delimited lists\n",
    "    key_points = CommaSeperatedListOutputParser().parse(response_text)\n",
    "    print(\"\\nParsed key points:\")\n",
    "    for i, point in enumerate(key_points, 1):\n",
    "        print(f\"{i}. {point.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e16d113",
   "metadata": {},
   "source": [
    "# Idea 02 AI Name Guru"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea34d00",
   "metadata": {},
   "source": [
    "# PromptTemplate and BaseOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0c77ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a master of naming. Please create 3 Japanese-style names that incorporate elements or themes related to Japan. Return ONLY the names in a comma-separated format without any additional explanation. Example: For Japanese names, common boy names include Taro and common girl names include Hanako.\n",
      "Raw response: Sakura Kaze, Ryuuya Tsubaki, Hotaru Umi\n",
      "Parsed names: ['Sakura Kaze', ' Ryuuya Tsubaki', ' Hotaru Umi']\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaSeperatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "\n",
    "    def parse(self, text: str) -> list[str]:\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\",\")\n",
    "    \n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")\n",
    "\n",
    "# Modify the prompt to explicitly require comma-separated output formatting\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"You are a master of naming. Please create 3 Japanese-style names that incorporate elements or themes related to {country}. \"\n",
    "    \"Return ONLY the names in a comma-separated format without any additional explanation. \"\n",
    "    \"Example: For Japanese names, common boy names include {boy_name} and common girl names include {girl_name}.\"\n",
    ")\n",
    "\n",
    "message = prompt.format(country=\"Japan\", boy_name=\"Taro\", girl_name=\"Hanako\")\n",
    "print(message)\n",
    "\n",
    "strs = llm.predict(message)\n",
    "print(\"Raw response:\", strs)\n",
    "\n",
    "names = CommaSeperatedListOutputParser().parse(strs)\n",
    "print(\"Parsed names:\", names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f55804d",
   "metadata": {},
   "source": [
    "# ChatPromptTemplate\n",
    "The template for the dialog has a structure， `chatmodels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6ab5a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a naming guru, your name is Ryuichi', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hello Ryuichi, how are you feeling today?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='I am feeling excellent today, thank you for asking', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a naming Meimeishi, your name is {name}\"),\n",
    "        (\"human\", \"Hello {name}, how are you feeling today?\"),\n",
    "        (\"ai\", \"I am feeling excellent today, thank you for asking\"),\n",
    "        (\"human\", \"{user_input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_template.format_messages(name=\"Ryuichi\", user_input=\"What is your name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa2992e",
   "metadata": {},
   "source": [
    "# 命名の達人  (Meimei no Tatsujin) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cfc0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import SystemMessage \n",
    "from langchain.schema import HumanMessage \n",
    "from langchain.schema import AIMessage \n",
    "\n",
    "# 直接创建消息\n",
    "SystemMessage(\n",
    "    content=\"You are a naming guru, your name is Meimeishi\",\n",
    "    additional_kwargs={\"Tatsujin_Name\": \"Meimeishi\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea60a437",
   "metadata": {},
   "outputs": [],
   "source": [
    "类方法，允许你从一个字符串模板创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fbbdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import AIMessagePromptTemplate\n",
    "from langchain.prompts import SystemMessagePromptTemplate\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain.prompts import ChatMessagePromptTemplate\n",
    "\n",
    "prompt = \"Dif-tor heh smusma-Live long and prosper {subject}\"\n",
    "\n",
    "chat_message_prompt = AIMessagePromptTemplate.from_template(template=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769467b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458b11f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
